\section[Mathematical Background: Linear Algebra]
	{Mathematical Background:\\Linear Algebra}%
\label{sec:linear_algebra}

This is intended as being a short crash course of linear algebra.

\subsection{Vector Spaces}%
\label{sub:vector_spaces}

A set $V$ is called a \textbf{linear space}
or a \textbf{vector space over the field} $\R$
if it is closed under vector summation and scalar multiplication.
\begin{align*}
	+ &: V \times V \rightarrow V \\
	\cdot &: \R \times V \rightarrow V
\end{align*}

With respect to addition ($+$) it forms a commutative group
(existence of neutral element $0$, inverse element $-v$).
Scalar multiplication respects the structure of $\R$ 
($\alpha(\beta v) = (\alpha \beta )v$).
Multiplication and addition respect the \textbf{distributive law}:
\begin{align*}
	( \alpha + \beta )v &= \alpha v + \beta v \\
	\alpha (u + v) & = \alpha u + \alpha v
\end{align*}

A subset $W \subset V$ of a vector space $V$
is called a \textbf{subspace} if $0 \in W$ and
$W$ is closed under $+$ and $\cdot$ (for all $\alpha \in \R$).\\

A \textbf{spanned subspace} of a set of vectors
$S = \{v_1, \hdots, v_k\} \subset V$ is the subspace
formed by all linear combinations of these vectors.
The set $S$ is called \textbf{linearly independent} if:
\[
	\sum_{i=1}^k \alpha_i v_i = 0 \implies \forall i, \alpha_i = 0
\]
A set $B = \{v_1, \hdots, v_n\}$ is called
a \textbf{basis} of $V$ if it is linearly independent and if it
spans the vector space $V$.
A basis is a maximal set of linearly independent vectors.


\subsubsection{Properties of a Basis}%
\label{ssub:properties_of_a_basis}

Let $B$ and $B'$ be two bases of a linear space $V$.

\begin{enumerate}
	\item $B$ and $B'$ contain the same number of vectors.
	This number $n$ is called the \textbf{dimension of the space} $V$.

\item Any vector $v \in V$ can be \textbf{uniquely expressed}
	as a linear combination of the basis vectors in
	$B = \{b_1, \hdots, b_n\}$:
	\[
		v = \sum_{i=1}^n \alpha_i b_i
	\]
	\item In particular, all vectors of $B'$ can be expressed as
	linear combinations of vectors of $B$:
	\[
		b'_j = \sum_{i=1}^n \alpha_{ij} b_i
	\]
	The coefficients $\alpha_{ij}$ for this \textbf{basis transform}
	can be combined in a matrix $A$:
	\[
		B' = BA \Leftrightarrow B = B'A^{-1}
	\]
\end{enumerate}


\subsubsection{Inner Product}%
\label{ssub:inner_product}

On a vector space, one can define an \textbf{inner product}
(or \textbf{dot product}):
\[\inner{\cdot}{\cdot} : V \times V \rightarrow \R\]
wich is defined by three properties:
\begin{enumerate}
	\item \textbf{linear}:
		$\inner{u}{\alpha v + \beta w} = \alpha \inner{u}{v} + \beta \inner{u}{w}$

	\item \textbf{symmetric}:
		$\inner{u}{v} = \inner{v}{u}$

	\item \textbf{positive definite}:\\
		$\inner{v}{v} \geq 0$ and $\inner{v}{v} = 0 \Leftrightarrow v = 0$
	
\end{enumerate}

The scalar product induces a \textbf{norm}:
\[|\cdot| : V \rightarrow \R, \quad |v| = \sqrt{\inner{v}{v}}\]

and a \textbf{metric}:
\[d : V \times V \rightarrow \R, \quad d(v,w) = |v - w| = \sqrt{\inner{v-w}{v-w}}\]

A vector space with an inner product is then also
a \textbf{metric space}. It is called a \textbf{Hilbert space}.


\subsubsection{Canonical and Induced Inner Product}%
\label{ssub:canonical_and_induced_inner_product}

On $V = \R^n$, one can define the canonical inner product for
the canonical basis $B = I_n$ as:

\[\inner{x}{y} = x^{\top} y = \sum_{i=1}^n x_i y_i\]

which induces the standard $L_2$-norm or Euclidean norm:

\[|x|_2 = \sqrt{x^{\top}x} = \sqrt{x_1^2 + \hdots + x_n^2}\]

With a basis transform $A$ to the new basis $B'$ given by
$B' = IA$ the canonical inner product in the new coordinates
$x'$, $y'$ is given by:

\[\inner{x}{y} = x^{\top} y = {(Ax')}^{\top} (Ay')
= x'^{\top} A^{\top} Ay' \equiv \inner{x'}{y'}_{A^{\top} A}\]

The latter product is called the \textbf{induced inner product}
from the matrix $A$.\\

Two vectors $v$ and $w$ are \textbf{orthogonal} iff\\
$\inner{v}{w} = 0$.


\subsubsection{Kronecker Product and Stack of a Matrix}%
\label{ssub:kronecker_product_and_stack_of_a_matrix}

Given two matrices $A \in \R^{m\times n}$ and $B \in \R^{k \times l}$,
one can define their \textbf{Kronecker product} by:

\[A \otimes B \equiv
	\begin{pmatrix}
		a_{11}B & \cdots & a_{1n}B \\
		\vdots & \ddots & \vdots \\
		a_{m1}B & \cdots & a_{mn}B \\
	\end{pmatrix}
\]
In matlab: $C = \mathrm{kron}(A,B)$.

Given a matrix $A \in \R^{m \times n}$, its \textbf{stack} $A^s$
is obtained by stacking its $n$ column vectors $a_1, \ldots, a_n \in \R^m$:

\[A^s \equiv
	\begin{pmatrix}
		a_1 \\
		\vdots \\
		a_n \\
	\end{pmatrix}
\in \R^{mn}\]

These notations allow to rewrite algebraic expressions, for example:

\[u^{\top} A v = {( v \otimes u )}^{\top} A^s\]

\subsection{Linear Transformations and Matrices}%
\label{sub:linear_transformations_and_matrices}

A \textbf{linear transformation} $L$ between tow linear spaces
$V$ and $W$ is a map $L : V \rightarrow W$ such that:
\begin{align*}
	\forall (x,y) \in V^2 ,\quad L(x+y) &= L(x) + L(y) \\
	\forall x \in V, \forall \alpha \in \R,\quad L(\alpha x) &= \alpha L(x)
\end{align*}

Due to the linearity, the action of $L$ on the space $V$
is uniquely defined by its action on the basis vectors of $V$.
In the canonical basis ${e_1, \ldots, e_n}$ we have:

\[\forall x \in V, \quad L(x) = Ax\]
where
\[A = (L(e_1), \ldots, L(e_n)) \in \R^{m \times n}\]

The set of all $m \times n$ matrices is denoted by $\M(m,n)$.
In case that $m = n$, the set $\M(m,n) \equiv \M(n)$
forms a \textbf{ring} over the field $\R$, i.e.\ it is closed
under matrix multiplication and summation.


\subsubsection{The Linear Groups $GL(n)$ and $SL(n)$}%
\label{ssub:the_linear_groups_gl_n_and_sl_n_}

A \textbf{group} is a set $G$ with an operation
$\circ : G \times G \rightarrow G$ such that:
\begin{enumerate}
	\item It is closed:
		$\forall (g_1, g_2) \in G^2, \quad g_1 \circ g_2 \in G$
	\item It is associative:
		$\forall (g_1, g_2, g_3) \in G^3, \quad
		(g_1 \circ g2) \circ g_3 = g_1 \circ (g_2 \circ g_3)$
	\item There is a neutral element:
		$\exists e \in G: \forall g \in G, \quad
		e \circ g = g \circ e = g$
	\item Each element has an inverse:
		$\forall g \in G, \exists \inv{g} \in G : \quad
		g \circ \inv{g} = \inv{g} \circ g = e$
\end{enumerate}

All invertible (non-singular) real $n \times n$ matrices
form a group with respect to matrix multiplication.
This group is called the \textbf{general linear group} $GL(n)$.
It consists of all $A \in \M(n)$ for which $\det(A) \ne 0$.\\

All matrices $A \in GL(n)$ for which $\det(A) = 1$ form a group
called the \textbf{special linear group} $SL(n)$.


\subsubsection{Matrix Representation of Groups}%
\label{ssub:matrix_representation_of_groups}

A group $G$ has a \textbf{matrix representation} or can be realized as
a matrix group if there exists an injective transformation:

\[R : G \rightarrow GL(n)\]

which \textbf{preserves the group structure} of $G$,
that is inverse and composition are preserved by the map:

\begin{align*}
	R(e) &= I_{n \times n} \\
	\forall (g,h) \in G^2, \quad R(g\circ h) &= R(g)R(h)
\end{align*}

Such a map $R$ is called a \textbf{group homomorphism}.
The idea is to study properties of the group by the matrices properties.


\subsubsection{The Affine Group $A(n)$}%
\label{ssub:the_affine_group_a_n_}

An affine transformation $L : \R^n \rightarrow \R^n$
is defined by a matrix $A \in GL(n)$ and a vector $b \in \R^n$ such that:
\[L(x) = Ax + b\]

The set of all such affine transformations is called the
\textbf{affine group of dimension $n$}, denoted by $A(n)$.
L defined above is not a linear map unless $b=0$.
By introducing \textbf{homogeneous coordinates} to represent $x \in \R^n$ by
$\inmatrix{x \\ 1} \in \R^{n+1}$,
$L$ becomes a linear mapping from:

\[L: \R^{n+1} \rightarrow \R^{n+1}; \quad
	\begin{pmatrix}
		x \\
		1 \\
	\end{pmatrix}
	\mapsto
	\begin{pmatrix}
		A & b \\
		0 & 1 \\
	\end{pmatrix}
	\begin{pmatrix}
		x \\
		1 \\
	\end{pmatrix}\]

A matrix $\inmatrix{A & b \\ 0 & 1}$ with $A \in GL(n)$ and $b \in R^n$
is called an \textbf{affine matrix}. It is an element of $GL(n+1)$.
The affine matrices form a subgroup of $GL(n+1)$.


\subsubsection{The Orthogonal Group $O(n)$}%
\label{ssub:the_orthogonal_group_o_n_}

A matrix $A \in \M(n)$ is called \textbf{orthogonal} if it preserves
the inner product:
\[\forall (x,y) \in {(\R^n)}^2 \quad \inner{Ax}{Ay} = \inner{x}{y}\]

The set of all orthogonal matrices forms the \textbf{orthogonal group}
$O(n)$, which is a subgroup of $GL(n)$.
For an orthogonal matrix $R$ we have:
\[\forall (x,y) \in {(\R^n)}^2, \quad
\inner{Rx}{Ry} = \tr{x}\tr{R}Ry = \tr{x}y\]

Therefore, we must have $\tr{R}R = R\tr{R} = I$, i.e.
\[O(n) = \{ R \in GL(n) \ | \ \tr{R}R = I \}\]

We can show that $\det(R) \in \{ \pm 1\}$.
The subgroup of $O(n)$ with $\det(R) = +1$ is called
the \textbf{special orthogonal group} $SO(n)$.
$SO(n) = O(n) \cap SL(n)$.
In particular, $SO(3)$ is the group of all 3-dimensional rotation matrices.


\subsubsection{The Euclidean Group $E(n)$}%
\label{ssub:the_euclidean_group_e_n_}

A Euclidean transformation $L$ from $\R^n$ to $\R^n$ is defined by
an orthogonal matrix $R \in O(n)$ and a vector $T \in R^n$:
\[L : \R^n \rightarrow \R^n; \quad x \mapsto Rx + T\]

The set of all such transformations is called the
\textbf{Euclidean group} $E(n)$. It is a subgroup of the affine group $A(n)$.
\[E(n) = \left\{
	\begin{pmatrix}
		R & T \\
		0 & 1 \\
	\end{pmatrix}
\ \middle| \ R \in O(n), \ T \in \R^n \right\}\]


If $R \in SO(n)$ then we have the \textbf{special euclidean group} $SE(n)$.
In particular, $SE(3)$ represent the \textbf{rigid-body motions} in $\R^3$.\\

In summary:

\[\boxed{SO(n) \subset O(n) \subset GL(n)}\]
\[\boxed{SE(n) \subset E(n) \subset A(n) \subset GL(n+1)}\]


\subsection{Properties of Matrices}%
\label{sub:properties_of_matrices}


\subsubsection{Range, Span, Null Space and Kernel}%
\label{ssub:range_span_null_space_and_kernel}

Let $A \in \Rmn$ be a matrix defining a linear map from $\Rn$ to $\Rm$.
The \textbf{range} or \textbf{span} of $A$ is defined as the subspace
of $\Rm$ which can be ``reached'' by $A$:
\[\text{range}(A) = \{ y \in \Rm \ |\  \exists x \in \Rn : Ax = y \}\]

The range of a matrix $A$ is given by the span of its column vectors.
The \textbf{null space} or \textbf{kernel} of a matrix $A$ is given by
the subset of vectors $x \in \Rn$ which are mapped to zero:
\[\text{null}(A) \equiv \ker(A) = \{ x \in \Rn \ | \ Ax = 0 \}\]

The null space of a matrix $A$ is given by the vectors
orthogonal to its row vectors. In matlab: $Z = \text{null}(A)$.
The concepts of range and null space are useful when studying the
\textbf{solution of linear equations}. The system $Ax = b$ will have
a solution $x \in \Rn$ if and only if $b \in \text{range}(A)$.
Moreover, this solution will be unique only if $\ker(A) = \{0\}$.\\

The \textbf{rank} of a matrix is the dimension of its range:
\[\text{rank}(A) = \dim(\text{range}(A))\]

The rank of a matrix $A \in \Rmn$ has the following properties:
\begin{enumerate}
	\item $\text{rank}(A) = n - \dim(\ker(A))$
	\item $0 \le \text{rank}(A) \le \min\{m,n\}$.
	\item $\text{rank}(A)$ is equal to the maximum number
		of linearly independent row (or column) vectors of $A$.
	\item $\text{rank}(A)$ is the highest order of a nonzero minor of $A$,
		where a \textbf{minor of order k} is the determinant
		of a $k \times k$ submatrix of $A$.
	\item \textbf{Sylvester's inequality}: Let $B \in \R^{n \times k}$.
		then $AB \in \R^{m \times k}$ and
		$\text{rank}(A) + \text{rank}(B) - n \le
			\text{rank}(AB) \le \min \{ \text{rank}(A), \text{rank}(B)\}$.
	\item For any nonsingular matrices $C \in \R^{m \times m}$
		and $D \in \R^{n \times n}$, we have:
		$\text{rank}(A) = \text{rank}(CAD)$.
\end{enumerate}


\subsubsection{Eigenvalues and Eigenvectors}%
\label{ssub:eigenvalues_and_eigenvectors}

Let $A \in \C^{n \times n}$ be a complex matrix.
A non-zero vector $v \in \C^n$ is called a \textbf{(right) eigenvector of $A$} if:
	\[\exists \lambda \in \C, \quad Av = \lambda v\]

$\lambda$ is called an \textbf{eigenvalue of $A$}.
Similarly $v$ is called a \textbf{left eigenvector} of $A$ if
$\tr{v}A = \lambda \tr{v}$ for some $\lambda \in \C$.

The \textbf{spectrum $\sigma(A)$ of a matrix $A$} is the set of all its eigenvalues.
In Matlab: $[V,D] = \text{eig} (A)$;

where $D$ is a diagonal matrix containing the eigenvalues and
$V$ is a matrix whose columns are the corresponding eigenvectors,
such that $AV = VD$.


\subsubsection{Properties of Eigenvalues and Eigenvectors}%
\label{ssub:properties_of_eigenvalues_and_eigenvectors}

Let $A \in \R^{n \times n}$ be a square matrix, then:
\begin{enumerate}
	\item If $Av = \lambda v$ for some $\lambda \in \R$, then there also exists a
		left-eigenvector $\eta \in \Rn \ : \ \tr{\eta}A = \lambda \tr{\eta}$.
		Hence $\sigma(A) = \sigma(\tr{A})$
	\item The eigenvectors of a matrix associated with different
		eigenvalues are linearly independent.
	\item All eigenvalues $\sigma(A)$ are the roots of the characteristic
		polynomial equation $\det(A - \lambda I) = 0$.
		Therefore $\det(A)$ is equal to the product of all eigenvalues
		(some of which may appear multiple times).
	\item If $B = PA\inv{P}$ for some nonsingular matrix $P$,
		then $\sigma(B) = \sigma(A)$
	\item If $\lambda \in \C$ is an eigenvalue, then its conjugate
		$\conj{\lambda}$ is also an eigenvalue.
		Thus $\sigma(A) = \conj{\sigma(A)}$ for real matrices $A$.
\end{enumerate}


\subsubsection{Symmetric Matrices}%
\label{ssub:symmetric_matrices}

A matrix $S \in \R^{n \times n}$ is called \textbf{symmetric} if $\tr{S} = S$.
A symmetric matrix S is called \textbf{positive semi-definite}
(denoted by $S \ge 0$) if:
	\[\forall x \ne 0, \quad \tr{x}Sx \ge 0\]

$S$ is called \textbf{positive definite} (denoted by $S>0$) if this is $>0$.
Let $S \in \R^{n \times n}$ be a real symmetric matrix. Then:
\begin{enumerate}
	\item All eigenvalues of $S$ are real, i.e. $\sigma(S) \subset \R$.
	\item Eigenvectors $v_i$ and $v_j$ of $S$ corresponding to distinct
		eigenvalues $\lambda_i \ne \lambda_j$ are orthogonal.
	\item There always exist $n$ orthonormal eigenvectors of $S$
		which form a basis of $\Rn$.
		Let $V = (v_1, \ldots, v_n) \in O(n)$ be the orthogonal matrix
		of these eigenvectors, and
		$\Lambda = \text{diag}\{\lambda_1, \ldots, \lambda_n\}$ the diagonal
		matrix of eigenvalues. Then we have $S = V \Lambda \tr{V}$.
	\item $S$ is positive (semi-) definite if all eigenvalues are
		positive (nonnegative).
	\item Let $S$ be positive semi-definite and $\lambda_1, \lambda_n$ the largest
		and smallest eigenvalues. Then $\lambda_1 = \max_{|x|=1} \inner{x}{Sx}$
		and $\lambda_n = \min_{|x|=1} \inner{x}{Sx}$.
	
\end{enumerate}


\subsubsection{Norms of Matrices}%
\label{ssub:norms_of_matrices}

There are many ways to define norms on the space of matrices $A \in \Rmn$.
They can be defined based on norms on the domain or codomain spaces
on which $A$ operates. In particular, the
\textbf{induced 2-norm of a matrix $A$} is defined as:
	\[\|A\|_2 \equiv \underset{|x|_2=1}{\max} |Ax|_2
	= \underset{|x|_2=1}{\max} \sqrt{\inner{x}{\tr{A}Ax}}\]

Alternatively, one can define the \textbf{Frobenius norm of $A$} as:
	\[\|A\|_f \equiv \sqrt{\sum_{i,j}a^2_{ij}}
	= \sqrt{\text{trace}(\tr{A}A)}\]

Note that these norms are in general not the same.
Since the matrix $\tr{A}A$ is symmetric and pos.\ semi-definite,
we can diagonalize it as:
\[\tr{A}A = V \text{diag}\{\sigma^2_1, \ldots, \sigma^2_n\}\tr{V}\]

with $\sigma^2_1 \ge \sigma^2_i \ge 0$. This leads to:
	\[\|A\|_2 = \sigma_1\]
	\[\|A\|_f = \sqrt{\text{trace}(\tr{A}A)} = \sqrt{\sigma^2_1 + \cdots + \sigma^2_n}\]

\subsubsection{Skew-symmetric Matrices}%
\label{ssub:skew_symmetric_matrices}

A matrix $A \in \R^{n \times n}$ is called
\textbf{skew-symmetric or anti-symmetric} if $\tr{A} = -A$.
If $A$ is a real skew-symmetric matrix, then:
\begin{enumerate}
	\item All eigenvalues of $A$ are either zero or purely imaginary,
		i.e.\ of the form $i\omega$ with $i ^2 = -1,\ \omega \in \R$.
	\item There exists an orthogonal matrix $V$ such that:
		\[A = V \Lambda \tr{V}\]
		where $\Lambda$ is a block-diagonal matrix.
		$\Lambda = \text{diag}\{A_1, \ldots, A_m, 0, \ldots, 0\}$,
		with real skew-symmetric matrices $A_i$ of the form:
		\[A_i = \begin{pmatrix}
				0 & a_i \\
				-a_i & 0
		\end{pmatrix}
		\in \R^{2 \times 2}, \quad i = 1, \ldots, m\]
		In particular, the rank of any skew-symmetric matrix is even.
\end{enumerate}


\subsubsection{Hat Operator}%
\label{ssub:hat_operator}

In Computer Vision, a common skew-symmetric matrix is given by the
\textbf{hat operator} of a vector $u \in \R^{3}$
	\[\widehat{u} = \begin{pmatrix}
		0 & -u_3 & u_2 \\
		u_3 & 0 & -u_1 \\
		-u_2 & u_1 & 0
	\end{pmatrix} \in \R^{3 \times 3}\]

This is a linear operator from the space of vectors $\R^3$
to the space of skew-symmetric matrices in $\RR{3}{3}$.
In particular, the matrix $\widehat{u}$ has the property that:
	\[\widehat{u}v = u \times v\]

where $\times$ denotes the standard vector cross product in $\R^3$.
For $u \ne 0$, we have $\text{rank}(\widehat{u}) = 2$ and the null space
of $\widehat{u}$ is spanned by u, because
$\widehat{u}u = \tr{u}\widehat{u} = 0$.


\subsection{The Singular Value Decomposition}%
\label{sub:the_singular_value_decomposition}

Many of matrix properties can be captured by the so-called
\textbf{singular value decomposition (SVD)}.
SVD can be seen as a generalization of eigenvalues and
eigenvectors to non-square matrices.
The computation of SVD is numerically well-conditioned.
It is very useful for solving linear-algebraic problems such as:
\begin{itemize}
	\setlength\itemsep{-0.2em}
	\item matrix inversion
	\item rank computation
	\item linear least-squares estimation
	\item projections
	\item fixed-rank approximations
\end{itemize}


\subsubsection{Algebraic Derivation of SVD}%
\label{ssub:algebraic_derivation_of_svd}

Let $A \in \Rmn$ with $m \ge n$ be a matrix of rank
$\text{rank}(A) = p$. Then there exist:
\begin{itemize}
	\item $U \in \RR{m}{p}$ whose columns are orthonormal,
	\item $V \in \RR{n}{p}$ whose columns are orthonormal, and
	\item $\Sigma \in \RR{p}{p}, \quad
		\Sigma = \text{diag}\{ \sigma_1, \ldots, \sigma_p \}$, with
		$\sigma_1 \ge \ldots \ge \sigma_p$
\end{itemize}
such that:
	\[A = U \Sigma \tr{V}\]

Note that this \textbf{generalizes the eigenvalue decomposition}.
While the latter decomposes a symmetric square matrix $A$ with
an orthogonal transformation $V$ as:
\[A = V \Lambda \tr{V}, \quad V \in O(n), \quad
\Lambda = \text{diag}\{\lambda_1, \ldots, \lambda_n\}\]

SVD allows to decompose an arbitrary (non-square) matrix $A$
of rank $p$ with two transformations $U$ and $V$ with orthonormal
columns as shown above.\\

The proof is quite interresting and relies on properties of $\tr{A}A$.
It is available in the second video, at minute 42.


\subsubsection{A Geometric Interpretation of SVD}%
\label{ssub:a_geometric_interpretation_of_svd}

For $A \in \RR{n}{n}$, the singular value decomposition
$A = U \Sigma \tr{V}$ is such that the columns $U = (u_1, \ldots, u_n)$
and $V = (v_1, \ldots, v_n)$ form orthonormal bases of $\Rn$.
If a point $x \in \Rn$ is mapped to a point $y \in \Rn$
by the transformation $A$, then the coordinates of $y$ in basis $U$
are related to the coordinates of $x$ in basis $V$
by the diagonal matrix $\Sigma$. Each coordinate is merely scaled by
the corresponding singular value:
\[y = Ax = U\Sigma\tr{V}x \quad \Leftrightarrow \quad \tr{U}y = \Sigma \tr{V}x\]

\textbf{The matrix $A$ maps the unit sphere into an ellipsoid with
semi-axis $\sigma_i u_i$}. To see this, we call $\alpha \equiv \tr{V}x$
the coefficients of the point $x$ in the basis $V$ and those of $y$
in basis $U$ shall be called $\beta \equiv \tr{U}y$.
All points of the circle fulfill $|x|^2_2 = \sum_i\alpha^2_i = 1$.
The above statement says that $\beta_i = \sigma_i\alpha_i$.
Thus for the points on the sphere we have:
	\[\sum_i\alpha^2_i = \sum_i\frac{\beta^2_i}{\sigma^2_i} = 1\]

which states that the transformed points lie on an ellipsoid
oriented along the axes of the basis $U$.


\subsubsection{The Generalized (Moore Penrose) Inverse}%
\label{ssub:the_generalized_moore_penrose_inverse}

For certain quadratic matrices, one can define an inverse matrix, if $\det(A) \ne 0$.
The set of all invertible matrices forms the group $GL(n)$.
One can also define a \textbf{(generalized) inverse}
(also called \textbf{pseudo inverse}) for an arbitrary (non-quadratic)
matrix $A \in \Rmn$. If its SVD is $A = U \Sigma \tr{V}$
the pseudo inverse is defined as:
	\[\pinv{A} = V \inv{\Sigma} \tr{U}\]

If we consider the ``non-economical'' SVD decomposition
(with $U \in \RR{m}{m}$, $\Sigma \in \Rmn$ (completed with 0), and $V \in \RR{n}{n}$),
then this can also be written as:
	\[\pinv{A} = V \pinv{\Sigma} \tr{U}, \quad
		\text{where}\ \pinv{\Sigma} =
		\begin{pmatrix}
			\inv{\Sigma_1} & 0 \\
			0 & 0
		\end{pmatrix}_{n \times m}\]

where $\Sigma_1$ is the diagonal matrix of non-zero singular values.
\textbf{In Matlab:} $X = pinv(A)$.
In particular, the pseudo inverse can be employed in similar fashion
as the inverse of quadratic invertible matrices:
	\[A \pinv{A} A = A, \quad \pinv{A} A \pinv{A} = \pinv{A}\]

The linear system $Ax = b$ with $A \in \Rmn$ of rank $r \le \min(m,n)$
can have multiple or no solutions.
$x_{\min} = \pinv{A}b$ \textbf{is among all minimizers of $|Ax-b|^2$
the one with the smalest norm $|x|$.}
